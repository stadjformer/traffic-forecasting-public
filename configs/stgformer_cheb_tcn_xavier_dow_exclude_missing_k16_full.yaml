# STGFormer with Chebyshev Propagation + TCN Temporal Mode + Xavier Init + DOW Embeddings
# Full 100-epoch training version of the final architecture with sparsity_k=16
#
# ** FINAL ARCHITECTURE - Best performing model across all experiments **
# Achieves state-of-the-art performance on METR-LA (MAE: 3.077 @ 1hr)
# and competitive performance on PEMS-BAY (MAE: 1.577 @ 1hr)
#
# This is the full training configuration (100 epochs) for final model evaluation.
#
# Usage:
#   python scripts/train_stgformer.py --config configs/stgformer_cheb_tcn_xavier_dow_exclude_missing_k16_full.yaml
#

description: "STGFormer Chebyshev+TCN with Xavier initialization, DOW embeddings, exclude_missing_from_norm, and sparsity_k=16 [FINAL - 100 epochs]"

datasets:
  - PEMS-BAY
  - METR-LA

model:
  num_layers: 3
  num_heads: 4
  input_embedding_dim: 24
  tod_embedding_dim: 24
  dow_embedding_dim: 24
  adaptive_embedding_dim: 80
  dropout: 0.1
  dropout_a: 0.3
  mlp_ratio: 4
  use_mixed_proj: true

propagation:
  mode: chebyshev

temporal:
  mode: tcn
  num_layers: 3
  kernel_size: 3
  dilation_base: 2
  dropout: 0.1

graph:
  mode: learned
  lambda_hybrid: 0.5
  sparsity_k: 16

training:
  epochs: 100
  batch_size: 200
  learning_rate: 0.001
  weight_decay: 0.0003
  early_stop: 10
  clip_grad: 0.0
  milestones: [20, 30]
  lr_decay_rate: 0.1
  seed: 42

initialization:
  use_zero_init: false

output:
  hf_repo_prefix: STGFORMER_FINAL

wandb:
  entity: cs224w-traffic-forecasting
  enabled: true

exclude_missing_from_norm: true
